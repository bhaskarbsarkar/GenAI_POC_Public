# from dotenv import load_dotenv
# load_dotenv()
import os
import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from agents.hyde_agent import HyDEAgent
from agents.response_agent import ResponseSynthesisAgent
from agents.followup_agent import FollowUpQuestionAgent
from pinecone_util import init_pinecone
from langgraph.graph import StateGraph, START, END
from typing import TypedDict, List

os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
os.environ["PINECONE_API_KEY"] = st.secrets["PINECONE_API_KEY"]

# Initialize OpenAI LLM
hyde_llm = ChatOpenAI(model_name="gpt-4o", max_completion_tokens=150)
response_llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.2)
followup_llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

# Initialize Pinecone and Index
index = init_pinecone()
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

# Define State
class MedicalChatbotState(TypedDict):
    query: str
    hypothetical_answer: str
    synthesized_answer: str
    follow_up_questions: List[str]
    chat_history: List[str]

# Initialize State
initial_state = MedicalChatbotState(
    query="",
    hypothetical_answer="",
    synthesized_answer="",
    follow_up_questions=[],
    chat_history=[]
)

# Initialize Agents
hyde_agent = HyDEAgent(hyde_llm)
response_synthesis_agent = ResponseSynthesisAgent(llm=response_llm, embedding=embeddings, pc_index=index)
follow_up_question_agent = FollowUpQuestionAgent(followup_llm)

# Define Workflow Graph
workflow = StateGraph(state_schema=MedicalChatbotState)
workflow.add_node("hyde_agent", hyde_agent.generate_hypothetical_answer)
workflow.add_node("response_agent", response_synthesis_agent.synthesize_response)
workflow.add_node("followup_agent", follow_up_question_agent.generate_follow_up_questions)

# Define the edges of the workflow graph
workflow.add_edge(START, "hyde_agent")
workflow.add_edge("hyde_agent", "response_agent")
workflow.add_edge("response_agent", "followup_agent")
workflow.add_edge("followup_agent", END)

# Compile the workflow into a chain
chain = workflow.compile()

# Streamlit UI
st.title("Medical Chatbot with Memory")

# Initialize conversation state in Streamlit session state
if "conversation" not in st.session_state:
    st.session_state.conversation = []

# Input field for user to enter medical question
user_input = st.text_input("Enter your medical question:")

if st.button("Ask"):
    if user_input:

        response = ""
        
        # Update initial state with user input and conversation history
        initial_state["query"] = user_input
        initial_state["chat_history"] = st.session_state.conversation
        
        # Invoke the workflow chain with the initial state
        output_state = chain.invoke(initial_state)

        # Display the hypothetical answer generated by hyde_agent
        st.subheader("HyDE Agent Response")
        st.write(output_state["hypothetical_answer"])
        
        # Display the synthesized answer generated by response_agent
        st.subheader("RAG Agent Response")
        st.write(output_state["synthesized_answer"])
        response += "\n".join(output_state["synthesized_answer"])

        # Display the follow-up questions generated by followup_agent
        st.subheader("Follow-Up Questions")
        for question in output_state["follow_up_questions"]:
            st.write(f"- {question}")
        response += "\n".join(output_state["follow_up_questions"])

        # Append the conversation to the session state
        st.session_state.conversation.append({"Human": user_input, "AI": response[:300]})

    else:
        st.warning("Please enter a medical question.")
